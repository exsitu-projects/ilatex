% !TEX root = main.tex

\section{Application: Solving PDEs}

\begin{table}[!t]
	\centering
	\begin{threeparttable}
		\begin{tabular}{lcc}
					\hline
					& $\nu=10^{-4}$ & $\nu=10^{-5}$ \\
					Method                     & $T=30$ & $T=20$  \\ 
					\hline
					%FNO-3D \citep{li2021fno}                &    0.1918    &    0.1893   \\
					%FNO-2D \citep{li2021fno}                &     {\bf 0.1559}   &   {\bf 0.1556}    \\
					%U-Net \citep{li2021fno}                &     0.2051   &    0.1982   \\
					%TF-Net \citep{li2021fno}                &     0.2253   &   0.2268    \\
					%ResNet \citep{li2021fno}                &    0.2871    &   0.2753    \\
					%\hline
					CNN-3d backbone                 &    0.325    &  0.278    \\
					FNO-3d (reproduced)                           &   0.182     &  0.177   \\
					{\bf CNN-3d backbone XD}  &     {\bf 0.172}  &  {\bf 0.168}     \\
					\hline
				\end{tabular}
%		\begin{tablenotes}\footnotesize
%			\item[$\ast$] No data augmentation used in the permuted case.
%			\item[$\dagger$] Training using ``base" routine from \citet{yang2020nas}.
%		\end{tablenotes}
		\caption{\label{tab:navierstokes}
			Relative test error on the 2d Navier-Stokes equations at two different settings of the viscosity $\nu$ and number of time steps $T$. 
			Best results in each setting are {\bf bolded}.
		}
	\end{threeparttable}
\end{table}

\begin{table*}[!t]
	\centering
	\begin{threeparttable}
		\begin{tabular}{lcccc}
			\hline
			& Permuted MNIST$^\ast$ & ~JSB Chorales~~ & ~~~~Nottingham~~~~ & ~~~Penn Treebank \\
			Method (source)  & (error) & (loss) & (loss) & (perplexity) \\
			\hline
			LSTM \citep{bai2018tcn} & 14.3 & 8.45 & 3.29 & 78.93 \\
			GRU \citep{bai2018tcn} & 12.7 & 8.43 & 3.46 & 92.48 \\
			RNN \citep{bai2018tcn} & 74.7 & 8.91 & 4.05 & 114.50 \\
			TCN backbone \citep{bai2018tcn} & 2.8 & 8.10 & 3.07 & 88.68 \\
			TrellisNet \citep{bai2019trellis} & 1.87 & - & - & {\bf 54.19} \\
			R-Transformer \citep{wang2020rtransformer} & - & - & {\bf 2.37} & 84.38 \\
			HiPPO-LegS \citep{gu2020hippo} & {\bf 1.7} & - & - & - \\
			\hline
			TCN backbone (reproduced) & $2.89\pm0.04$ & $8.17\pm0.01$ & $2.97\pm0.01$ & $88.49\pm0.31$ \\
			{\bf TCN backbone XD} (ours) & ${\bf 1.75\pm0.11}$ & ${\bf 8.07\pm0.02}$ & $2.81\pm0.05$ & $84.11\pm0.25$ \\
			\hline
		\end{tabular}
		\begin{tablenotes}\footnotesize
			\item[$\ast$] We use depth $\*d=(3,3,3)$ XD-operations for permutations;
			elsewhere we use $(1,3,1)$ to warm-start with dilated convolutions.\vspace{-4pt}
		\end{tablenotes}
		\caption{\label{tab:seq}
			XD-operations applied to TCNs compared to recent empirical results in sequence modeling.
			Our results are averages of three trials.
			Methods achieving within one deviation of the best performance are {\bf bolded}.
		}
	\end{threeparttable}
\end{table*}

As our first non-vision application, we consider the task of solving PDEs, an important application area of ML in the natural sciences \citep{li2015butterfly, li2018multidimensional, sirignano2018dgm}.
In our setup, data generated by classical PDE solvers is used to learn functions from some initial condition or setting to the corresponding PDE solution, with the goal of replacing the solver by a deep net forward pass;
the latter can be orders of magnitude faster.
Specifically, we evaluate using the setting of \citet{li2021fno}, whose Fourier Neural Operator is a recent method that significantly improves upon previous neural approaches, including baseline convolutional networks (CNNs).
The three PDEs they study are the Burgers' equation, Darcy Flow, and the 2d Navier-Stokes equations, which involve 1d, 2d, and 3d data, respectively.
The first two are studied across multiple resolutions, while the last one is studied at different viscosities.

%As a setup, we consider the same setting as that of \citet{li2021fno}, specifically the Burgers' equation, Darcy Flow, and the Navier-Stokes equations;
%as the data domain of these tasks is 1d, 2d, and 3d, respectively, this application also allows us to examine the performance of XD-operations across several dimensionalities. We follow the setup from \citet{li2021fno} and evaluate on the same 1d Burgers', 2d Darcy Flow, and 2d Navier-Stokes datasets used in their experiments. Per \citet{li2021fno}, the 2d Navier-Stokes problem involves 3d data which treats time as a third dimension. 

Our approach here will be to take a simple CNN backbone---the type a scientist might use in a first attempt at a solution---and replace all convolutions by XD-operations.
As a baseline, we hope to do better than this backbone;
ambitiously, we hope to compete with the custom-designed FNO.
The specific CNN we use is simply the FNO architecture of the appropriate dimension $N$ from \citet{li2021fno} but with all $N$-dimensional FNOs replaced by $N$-dimensional convolutions;
this performs similarly to their CNN baselines.
In all cases we only compare to the CNN backbone and our reproduction of the FNO results, as the latter exceeds all other neural methods;
a complete results table is provided in the appendix.
Our reproduction of FNO is slightly worse than their reported numbers for Burgers' equation and slightly better in the other two settings.
Lastly, note that on the Navier-Stokes equations we only compare to the 3d FNO on the two settings in which we were able to reproduce their approach;
moreover, we do {\em not} compare to their use of a 2d FNO plus a recurrent net in time, but in-principle XD-operations can also be substituted into the latter.

For evaluating XD-operations we again follow the procedure in Section~\ref{sec:chrysalis}, in which we tune only the architecture optimizer;
notably, we do this only at the lowest resolutions.
At all dimension we use XD-operations of depth $\*d=\1_3$;
in addition, in dimensions $N>1$ we fix the architecture biases $\*b$ and channel gates $\*C$ to $\0$ and $\1$, respectively, to conserve memory at higher resolutions.
At lower resolutions we find that the performance difference is negligible.

We report our results for the Burger's equation and Darcy Flow in Figure~\ref{fig:pde};
for 2d Navier-Stokes the results are in Table~\ref{tab:navierstokes}.
In all cases we dramatically outperform the $N$-dimensional CNN backbone used to initialize XD-operations;
furthermore, we also achieve better error than the FNO, despite the latter being custom-made for this problem.
As exemplified in Figure~\ref{fig:pde}, despite having a higher train loss than FNO, XD-operations seem to generalize better.
Finally, Figure~\ref{fig:pde} also shows that XD-operations perform consistently well across different resolutions, a major advantage of FNOs over previous methods, whose performance was tightly coupled to the discretization \citep{li2021fno}.
In particular, note that CNN performance worsens with increasing resolution, whereas that of XD and FNO does not.

These results provide strong evidence that XD-operations are a useful search space for discovering neural operations, even in domains where the convolutions used to initialize them perform much worse than state-of-the-art methods.
Note that in this setting our results do come at a cost of slower training and inference;
XD-operations are roughly an order of magnitude slower than FNOs, despite having fewer parameters in 2d and 3d.
However, this still yields approximate PDE solvers that are one-to-two orders of magnitude faster than classical approaches, maintaining the usefulness of these results for the problem.