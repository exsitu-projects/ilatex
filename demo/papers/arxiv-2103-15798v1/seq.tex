% !TEX root = main.tex

\section{Application: Sequence Modeling}\label{sec:seq}

Our final application is in sequence modeling, in which the goal is prediction from 1d data.
While its text application is heavily studied in ML, there are numerous other applications such as in music \citep{allan2005chorales} and biology \citep{chen2019biological}.
The dominant approaches are recurrent nets \citep{hochreiter1997lstm} and Transformers \cite{vaswani2017attention}, but recent work has shown that models with dilated temporal convolutions are also competitive \citep{bai2018tcn,bai2019trellis}.
We will use the simple TCN of \citet{bai2018tcn} as a backbone network to examine the potential for XD-operation search to improve performance.
Our primary goal will be to exceed the TCN baseline across several domains:
flat permuted images, music, and text.
We also compare to more recent, human-designed architectures.

We choose evaluation tasks from those used by \citet{bai2018tcn} to study TCNs.
As before, we use the same network size and model weight optimization as the backbone, and we initialize XD-operation using its operations, in this case dilated temporal convolutions.
As discussed in Section~\ref{subsec:express}, to handle dilations we simply need the middle K-matrix $\*L$ to have depth 3.
Notably, we can painlessly handle dilation size growing exponentially with layer number as in TCNs, while standard NAS operation spaces like DARTS only contain dilations of size 1 and 2.
More difficult to handle is causality enforcement: 
making sure the input data does not contain the target when predicting the next entry.
We do this in a brute-force manner by treating sequences of length $n$ as $n-1$ data-points with masked targets.
In contrast, TCNs can use temporal shifts and are thus much more efficient to train.
We discuss this drawback at the end of the section, for now noting only that it limits our evaluation of XD-operations to four smaller datasets from \citet{bai2018tcn}.

Our results are presented in Table~\ref{tab:seq} and show that using XD-operations improves TCN performance on all four tasks.
Notably, our method is competitive with several more sophisticated approaches, exceeding TrellisNet on permuted MNIST---where we match the best-known result, HiPPO-LegS---and improving upon R-Transformer on Penn Treebank (PTB).
Note that TrellisNet, whose PTB performance exceeds that of the best recurrent NAS cells \citep{bai2019trellis}, also uses convolutions and thus may be improved by XD-operations;
we do not evaluate this due to its training cost (TrellisNet is 2.5 times larger than the TCN).

Together with our results on image classification and PDEs, our study of sequence modeling provides further evidence that XD-operations can effectively find good operations using standard backbones on diverse types of data.
As noted above, a major limitation here is that XD-operations do not inherit the efficient causality-handling of convolutions.
One alternative approach is to study applications where the full sequence is available, e.g. machine translation \citep{stahlberg2020nmt} or question answering \citep{rajpurkar2016squad}.
However, it may also be possible to directly enforce causality in XD-operations by forcing architecture parameters $\*K$ and $\*M$ to be lower triangular;
since a product of lower triangular matrices is again lower triangular, the entire operation is then a multiplication of the input sequence by a lower triangular matrix, which suffices to prevent causality violations.